Pruning deep neural networks is an effective strategy to reduce the overall memory and compute requirements of these models (Narang et al., 2017; Han et al., 2015). However, these approaches induce
random, unstructured sparsity in the weight matrices. Speed-up obtained with random sparsity on various hardware platforms are lower than expected (as shown in Narang et al. (2017); Narang
& Diamos (2017)). Sparse formats do not efficiently utilize the hardware resources due to storage overheads, irregular memory access, and inability to take advantage of array data-paths in modern
processors.


